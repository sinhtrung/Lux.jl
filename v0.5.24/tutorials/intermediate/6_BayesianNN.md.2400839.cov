   157503 ```@meta
   265628 EditURL = "../../../../examples/BayesianNN/main.jl"
   157503 ```
  1102521 
   945038 # Bayesian Neural Network
   945038 
   945038 We borrow this tutorial from the
       20 [official Turing Docs](https://turing.ml/dev/tutorials/03-bayesian-neural-network/). We
   157503 will show how the explicit parameterization of Lux enables first-class composability with
        - packages which expect flattened out parameter vectors.
        - 
        - We will use [Turing.jl](https://turing.ml) with [Lux.jl](https://lux.csail.mit.edu/stable)
    50004 to implement implementing a classification algorithm. Lets start by importing the relevant
    50003 libraries.
        - 
        - ````julia
    50003 # Import libraries
   100006 using Lux, Turing, CairoMakie, Random, Tracker, Functors, MakiePublication, LinearAlgebra
        - 
        - # Sampling progress
    50003 Turing.setprogress!(true);
        - 
        - # Use reverse_diff due to the number of parameters in neural networks
    50003 Turing.setadbackend(:tracker)
        - ````
        - 
        - ````
        4 [ Info: [Turing]: progress logging is enabled globally
        4 [ Info: [AdvancedVI]: global PROGRESS is set as true
        4 ┌ Warning: `ADBACKEND` and `setbackend` are deprecated. Please specify the chunk size directly in the sampler constructor, e.g., `HMC(0.1, 5; adtype=AutoForwardDiff(; chunksize=0))`.
        4 │  This function has no effects.
        4 │   caller = ip:0x0
        - └ @ Core :-1
        4 
        4 ````
        4 
        - ## Generating data
        4 
        4 Our goal here is to use a Bayesian neural network to classify points in an artificial dataset. The code below generates data points arranged in a box-like pattern and displays a graph of the dataset we'll be working with.
        - 
        4 ````julia
        - # Number of points to generate
        - N = 80
        4 M = round(Int, N / 4)
        - rng = Random.default_rng()
        - Random.seed!(rng, 1234)
        - 
        - # Generate artificial data
        - x1s = rand(rng, Float32, M) * 4.5f0;
        - x2s = rand(rng, Float32, M) * 4.5f0;
        - xt1s = Array([[x1s[i] + 0.5f0; x2s[i] + 0.5f0] for i in 1:M])
        - x1s = rand(rng, Float32, M) * 4.5f0;
        - x2s = rand(rng, Float32, M) * 4.5f0;
        - append!(xt1s, Array([[x1s[i] - 5.0f0; x2s[i] - 5.0f0] for i in 1:M]))
        - 
        - x1s = rand(rng, Float32, M) * 4.5f0;
        - x2s = rand(rng, Float32, M) * 4.5f0;
        - xt0s = Array([[x1s[i] + 0.5f0; x2s[i] - 5.0f0] for i in 1:M])
        - x1s = rand(rng, Float32, M) * 4.5f0;
        - x2s = rand(rng, Float32, M) * 4.5f0;
        - append!(xt0s, Array([[x1s[i] - 5.0f0; x2s[i] + 0.5f0] for i in 1:M]))
        - 
        - # Store all the data for later
        - xs = [xt1s; xt0s]
        - ts = [ones(2 * M); zeros(2 * M)]
        - 
        - # Plot data points
        - 
        - function plot_data()
        -     x1 = first.(xt1s)
        -     y1 = last.(xt1s)
        -     x2 = first.(xt0s)
        -     y2 = last.(xt0s)
        - 
        -     fig = with_theme(theme_web()) do
        -         fig = Figure()
        -         ax = CairoMakie.Axis(fig[1, 1]; xlabel="x", ylabel="y")
        - 
        -         scatter!(ax, x1, y1; markersize=8, color=:red, strokecolor=:black, strokewidth=1)
        -         scatter!(ax, x2, y2; markersize=8, color=:blue, strokecolor=:black, strokewidth=1)
        - 
        -         return fig
        -     end
        - 
        -     return fig
        - end
        - 
        - plot_data()
        - ````
        - 
        - ```@raw html
        - <img width=600 height=408 style='object-fit: contain; height: auto;' src="data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAABLAAAAMwCAIAAAC/VA7OAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdd0CVZePG8euAoIgCiuIExZEjzdyouVJya5ory7KyNLNhpWVZjl7N1DIzzVGvK0f6uvdKc49MJVFzK6QIggIKMs/vD34ZKoes4Hk4nO/nr3rum3MuII2L+3nu22K1WgUAAAAAcDxOZgcAAAAAAJiDQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOikIIAAAAAA6KQggAAAAADopCCAAAAAAOKo/ZAXIhi8VidgQAAAAAuYTVas2+F2eFEAAAAAAcFCuE2SVbezwAAACAXM+Aew9ZIQQAAAAAB0UhBAAAAAAHRSEEAAAAAAdFIQQAAAAAB0UhBAAAAAAHRSEEAAAAAAdFIQQAAAAAB0UhBAAAAAAHRSEEAAAAAAdFIQQAAAAAB0UhBAAAAAAHRSEEAAAAAAdFIQQAAAAAB0UhBAAAAAAHlcfsALlW5cqV7/zzyZMnTUwCAAAAABmyWK1WszPkNhaL5Z4rfJEBAAAA/F1pzSJb2wQrhNnlxIkTZkcAAAAAgMywQpj1DOjxAAAAAHI9A5oFm8oAAAAAgIOiEAIAAACAg6IQAgAAAICDohACAAAAgINil1HACAkJCQsXLly2bNnJky
        - ```
        - 
        - ## Building the Neural Network
        - 
        - The next step is to define a feedforward neural network where we express our parameters as
        - distributions, and not single points as with traditional neural networks. For this we will
        - use `Dense` to define liner layers and compose them via `Chain`, both are neural network
        - primitives from `Lux`. The network `nn` we will create will have two hidden layers with
        - `tanh` activations and one output layer with `sigmoid` activation, as shown below.
        - 
        - The `nn` is an instance that acts as a function and can take data, parameters and current
        - state as inputs and output predictions. We will define distributions on the neural network
        - parameters.
        - 
        - ````julia
        - # Construct a neural network using Lux
        - nn = Chain(Dense(2 => 3, tanh), Dense(3 => 2, tanh), Dense(2 => 1, sigmoid))
        - 
        - # Initialize the model weights and state
        - ps, st = Lux.setup(rng, nn)
        - 
        - Lux.parameterlength(nn) # number of paraemters in NN
        - ````
        - 
        - ````
        - 20
        - ````
        - 
        - The probabilistic model specification below creates a parameters variable, which has IID
        - normal variables. The parameters represents all parameters of our neural net (weights and
        - biases).
        - 
        - ````julia
        - # Create a regularization term and a Gaussian prior variance term.
        - alpha = 0.09
        - sig = sqrt(1.0 / alpha)
        - ````
        - 
        - ````
        - 3.3333333333333335
        - ````
        - 
        - Construct named tuple from a sampled parameter vector. We could also use ComponentArrays
        - here and simply broadcast to avoid doing this. But let's do it this way to avoid
        - dependencies.
        - 
        - ````julia
        - function vector_to_parameters(ps_new::AbstractVector, ps::NamedTuple)
        -     @assert length(ps_new) == Lux.parameterlength(ps)
        -     i = 1
        -     function get_ps(x)
        -         z = reshape(view(ps_new, i:(i + length(x) - 1)), size(x))
        -         i += length(x)
        -         return z
        -     end
        -     return fmap(get_ps, ps)
        - end
        - 
        - # Specify the probabilistic model.
        - @model function bayes_nn(xs, ts)
        -     global st
        - 
        -     # Sample the parameters
        -     nparameters = Lux.parameterlength(nn)
        -     parameters ~ MvNormal(zeros(nparameters), Diagonal(abs2.(sig .* ones(nparameters))))
        - 
        -     # Forward NN to make predictions
        -     preds, st = nn(xs, vector_to_parameters(parameters, ps), st)
        - 
        -     # Observe each prediction.
        -     for i in 1:length(ts)
        -         ts[i] ~ Bernoulli(preds[i])
        -     end
        - end
        - ````
        - 
        - ````
        - bayes_nn (generic function with 2 methods)
        - ````
        - 
        - Inference can now be performed by calling sample. We use the HMC sampler here.
        - 
        - ````julia
        - # Perform inference.
        - N = 5000
        - ch = sample(bayes_nn(reduce(hcat, xs), ts), HMC(0.05, 4), N)
        - ````
        - 
        - ````
        - Chains MCMC chain (5000×30×1 Array{Float64, 3}):
        - 
        - Iterations        = 1:1:5000
        - Number of chains  = 1
        - Samples per chain = 5000
        - Wall duration     = 22.97 seconds
        - Compute duration  = 22.97 seconds
        - parameters        = parameters[1], parameters[2], parameters[3], parameters[4], parameters[5], parameters[6], parameters[7], parameters[8], parameters[9], parameters[10], parameters[11], parameters[12], parameters[13], parameters[14], parameters[15], parameters[16], parameters[17], parameters[18], parameters[19], parameters[20]
        - internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, numerical_error, step_size, nom_step_size
        - 
        - Summary Statistics
        -       parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   ess_per_sec
        -           Symbol   Float64   Float64   Float64    Float64    Float64   Float64       Float64
        - 
        -    parameters[1]   -3.2838    2.1353    0.6061    13.4569    20.5157    1.3553        0.5857
        -    parameters[2]   -2.5453    2.0214    0.5200    14.3062    38.1263    1.2127        0.6227
        -    parameters[3]    0.2470    0.5170    0.1303    15.5400    69.9446    1.5131        0.6764
        -    parameters[4]   -1.1252    1.3161    0.3497    12.6263    27.1661    1.6134        0.5496
        -    parameters[5]   -1.8002    1.6926    0.4788    12.6602    33.8961    1.5684        0.5511
        -    parameters[6]    3.9672    2.1695    0.5954    14.0390    38.7946    1.3993        0.6111
        -    parameters[7]   -2.5286    2.1975    0.6476    12.0525    41.6970    1.6312        0.5246
        -    parameters[8]    1.8982    2.2701    0.6251    13.7761    28.7269    1.0725        0.5996
        -    parameters[9]   -1.2549    1.3480    0.2755    23.6932    59.3599    1.0377        1.0313
        -   parameters[10]   -4.2255    1.5446    0.3788    16.9719    57.0242    1.2471        0.7387
        -   parameters[11]   -4.6723    1.9392    0.5368    14.1247    53.9306    1.1332        0.6148
        -   parameters[12]    0.4250    2.4120    0.6992    12.3128    21.2133    1.5221        0.5359
        -   parameters[13]    2.3588    1.2102    0.2731    20.9996    48.8714    1.0347        0.9141
        -   parameters[14]   -0.4172    3.8391    1.1613    11.6401    28.0804    1.7871        0.5067
        -   parameters[15]   -2.1761    1.6085    0.4487    13.4087    25.6770    1.1828        0.5836
        -   parameters[16]    0.9780    2.6501    0.7593    12.8452    25.3699    1.3569        0.5591
        -   parameters[17]   -2.9359    1.1614    0.2219    29.2929    32.1933    1.0702        1.2750
        -   parameters[18]    2.7888    2.9729    0.8878    11.7797    20.3006    1.6940        0.5127
        -   parameters[19]   -5.9399    1.5947    0.4036    16.3451    27.9753    1.0555        0.7115
        -   parameters[20]   -3.5665    2.1457    0.6262    12.2139    27.8688    1.7557        0.5316
        - 
        - Quantiles
        -       parameters      2.5%     25.0%     50.0%     75.0%     97.5%
        -           Symbol   Float64   Float64   Float64   Float64   Float64
        - 
        -    parameters[1]   -8.9072   -4.7464   -2.5269   -1.7119   -0.7331
        -    parameters[2]   -7.6220   -3.8540   -2.3673   -1.0140    0.5061
        -    parameters[3]   -1.0205   -0.0489    0.3157    0.6049    1.0431
        -    parameters[4]   -5.7669   -1.6373   -1.0735   -0.2033    0.6782
        -    parameters[5]   -5.8369   -2.9621   -1.4632   -0.6225    0.7054
        -    parameters[6]    0.6462    2.3803    3.5608    5.1908    9.2960
        -    parameters[7]   -6.3413   -4.3782   -2.5000   -0.6308    1.3728
        -    parameters[8]   -2.0000    0.1196    1.9379    3.2783    6.4703
        -    parameters[9]   -3.4560   -2.1832   -1.3622   -0.4546    1.9927
        -   parameters[10]   -7.1485   -5.3845   -4.2239   -3.0170   -1.5034
        -   parameters[11]   -7.7894   -6.4005   -4.7019   -2.9667   -1.4084
        -   parameters[12]   -3.5029   -1.4492    0.2097    2.3226    5.4709
        -   parameters[13]    0.4124    1.4219    2.2479    3.1261    4.9592
        -   parameters[14]   -6.7024   -3.5548   -1.2754    3.5667    5.6695
        -   parameters[15]   -4.8862   -3.1688   -2.3131   -1.2205    1.4772
        -   parameters[16]   -3.5627   -1.0369    0.8253    2.8720    6.6413
        -   parameters[17]   -5.6450   -3.6644   -2.8345   -2.1178   -0.9015
        -   parameters[18]   -4.3030    0.9028    2.5158    5.1998    7.5944
        -   parameters[19]   -8.4777   -7.1883   -5.9664   -4.9567   -2.5945
        -   parameters[20]   -7.5701   -5.3241   -3.5499   -1.8065   -0.2083
        - 
        - ````
        - 
        - Now we extract the parameter samples from the sampled chain as θ (this is of size
        - `5000 x 20` where `5000` is the number of iterations and `20` is the number of
        - parameters). We'll use these primarily to determine how good our model's classifier is.
        - 
        - ````julia
        - # Extract all weight and bias parameters.
        - θ = MCMCChains.group(ch, :parameters).value;
        - ````
        - 
        - ## Prediction Visualization
        - 
        - ````julia
        - # A helper to run the nn through data `x` using parameters `θ`
        - nn_forward(x, θ) = first(nn(x, vector_to_parameters(θ, ps), st))
        - 
        - # Plot the data we have.
        - fig = plot_data()
        - 
        - # Find the index that provided the highest log posterior in the chain.
        - _, i = findmax(ch[:lp])
        - 
        - # Extract the max row value from i.
        - i = i.I[1]
        - 
        - # Plot the posterior distribution with a contour plot
        - x1_range = collect(range(-6; stop=6, length=25))
        - x2_range = collect(range(-6; stop=6, length=25))
        - Z = [nn_forward([x1, x2], θ[i, :])[1] for x1 in x1_range, x2 in x2_range]
        - contour!(x1_range, x2_range, Z)
        - fig
        - ````
        - 
        - ```@raw html
        - <img width=600 height=408 style='object-fit: contain; height: auto;' src="data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAABLAAAAMwCAIAAAC/VA7OAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdZ2CUZfY28OuZXlJm0hskkFCkIyIIUkQEDFiRFf8uKioqKoquoq5rBUF2saBiQVTwFQUEV0RAQIpUQZBeNICUEEhCkpnUydT3wyRsZvI8kwSSmQlz/T6589yTOXEVc3LOfY7gcrlAREREREREoUcW6ACIiIiIiIgoMJgQEhERERERhSgmhERERERERCGKCSEREREREVGIYkJIREREREQUopgQEhERERERhSgmhERERERERCGKCSEREREREVGIYkJIREREREQUopgQEhERERERhSgmhERERERERCGKCSEREREREVGIYkJIREREREQUopgQEhERERERhSgmhERERERERCGKCSEREREREVGIYkJIREREREQUopgQEhERERERhSgmhERERERERCGKCSEREREREVGIYkJIREREREQUopgQEhERERERhSgmhERERERERCGKCSEREREREVGIYkJIREREREQUopgQEhERERERhSgmhERERERERCGKCSEREREREVGIYkJIREREREQUopgQEhERERERhShFoAO4DAmCEOgQiIiIiIjoMuFyuZrui7NCSEREREREFKJYIWwqTZrHExERERHRZc8PvYesEBIREREREYUoJoREREREREQhigkhERERERFRiGJCSEREREREFKKYEBIREREREYUoJoREREREREQhigkhERERERFRiGJCSEREREREFKKYEBIREREREYUoJoREREREREQhigkhERERERFRiGJCSEREREREFKKYEBIREREREYUoJoREREREREQhigkhERERERFRiGJCSEREREREFKKYEBIREREREYUoJoREREREREQhig
        - ```
        - 
        - The contour plot above shows that the MAP method is not too bad at classifying our data.
        - Now we can visualize our predictions.
        - 
        - $p(\tilde{x} | X, \alpha) = \int_{\theta} p(\tilde{x} | \theta) p(\theta | X, \alpha) \approx \sum_{\theta \sim p(\theta | X, \alpha)}f_{\theta}(\tilde{x})$
        - 
        - The `nn_predict` function takes the average predicted value from a network parameterized
        - by weights drawn from the MCMC chain.
        - 
        - ````julia
        - # Return the average predicted value across multiple weights.
        - nn_predict(x, θ, num) = mean([first(nn_forward(x, view(θ, i, :))) for i in 1:10:num])
        - ````
        - 
        - ````
        - nn_predict (generic function with 1 method)
        - ````
        - 
        - Next, we use the `nn_predict` function to predict the value at a sample of points where
        - the x1 and x2 coordinates range between -6 and 6. As we can see below, we still have a
        - satisfactory fit to our data, and more importantly, we can also see where the neural
        - network is uncertain about its predictions much easier---those regions between cluster
        - boundaries.
        - 
        - Plot the average prediction.
        - 
        - ````julia
        - fig = plot_data()
        - 
        - n_end = 1500
        - x1_range = collect(range(-6; stop=6, length=25))
        - x2_range = collect(range(-6; stop=6, length=25))
        - Z = [nn_predict([x1, x2], θ, n_end)[1] for x1 in x1_range, x2 in x2_range]
        - contour!(x1_range, x2_range, Z)
        - fig
        - ````
        - 
        - ```@raw html
        - <img width=600 height=408 style='object-fit: contain; height: auto;' src="data:image/png;base64, iVBORw0KGgoAAAANSUhEUgAABLAAAAMwCAIAAAC/VA7OAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdZ1gUVxcH8DOzhV1YepUigogioKCAXRM0GktU7A07dhNjYtTENKMx5rVH7L33rokaexfUWLBhoUrvbfu+H1YTRRZx2aW4/98nn8zlzh2fyM7Ze+45jEqlIgAAAAAAADA8bGUvAAAAAAAAACoHAkIAAAAAAAADhYAQAAAAAADAQCEgBAAAAAAAMFAICAEAAAAAAAwUAkIAAAAAAAADhYAQAAAAAADAQCEgBAAAAAAAMFAICAEAAAAAAAwUAkIAAAAAAAADhYAQAAAAAADAQCEgBAAAAAAAMFAICAEAAAAAAAwUAkIAAAAAAAADhYAQAAAAAADAQCEgBAAAAAAAMFAICAEAAAAAAAwUAkIAAAAAAAADhYAQAAAAAADAQCEgBAAAAAAAMFAICAEAAAAAAAwUAkIAAAAAAAADhYAQAAAAAADAQCEgBAAAAAAAMFAICAEAAAAAAAwUAkIAAAAAAAADhYAQAAAAAADAQCEgBAAAAAAAMFAICAEAAAAAAAwUAkIAAAAAAAADxa3sBXyAGIap7CUAAAAAAMAHQqVS6W9y7BACAAAAAAAYKOwQ6ote43gAAAAAAPjgVUDuIXYIAQAAAAAADBQCQgAAAAAAAAOFgBAAAAAAAMBAISAEAAAAAAAwUAgIAQAAAAAADBQCQgAAAAAAAAOFgBAAAAAAAMBAISAEAAAAAAAwUAgIAQAAAAAADBQCQgAAAAAAAAOFgBAAAAAAAMBAISAEAAAAAAAwUAgIAQAAAAAADBQCQgAAAAAAAAOFgBAAAAAAAMBAISAEAAAAAAAwUAgIAQAAAAAADBQCQgAAAAAAAAOFgBAAAAAAAMBAISAEAA
        - ```
        - 
        - Suppose we are interested in how the predictive power of our Bayesian neural network
        - evolved between samples. In that case, the following graph displays an animation of the
        - contour plot generated from the network weights in samples 1 to 5,000.
        - 
        - ````julia
        - fig = plot_data()
        - Z = [first(nn_forward([x1, x2], θ[1, :])) for x1 in x1_range, x2 in x2_range]
        - c = contour!(x1_range, x2_range, Z)
        - record(fig, "results.gif", 1:250:size(θ, 1)) do i
        -     fig.current_axis[].title = "Iteration: $i"
        -     Z = [first(nn_forward([x1, x2], θ[i, :])) for x1 in x1_range, x2 in x2_range]
        -     c[3] = Z
        -     return fig
        - end
        - ````
        - 
        - ````
        - "results.gif"
        - ````
        - 
        - ![](results.gif)
        - 
        - ## Appendix
        - 
        - ````julia
        - using InteractiveUtils
        - InteractiveUtils.versioninfo()
        - if @isdefined(LuxCUDA) && CUDA.functional(); println(); CUDA.versioninfo(); end
        - if @isdefined(LuxAMDGPU) && LuxAMDGPU.functional(); println(); AMDGPU.versioninfo(); end
        - ````
        - 
        - ````
        - Julia Version 1.10.2
        - Commit bd47eca2c8a (2024-03-01 10:14 UTC)
        - Build Info:
        -   Official https://julialang.org/ release
        - Platform Info:
        -   OS: Linux (x86_64-linux-gnu)
        -   CPU: 48 × AMD EPYC 7402 24-Core Processor
        -   WORD_SIZE: 64
        -   LIBM: libopenlibm
        -   LLVM: libLLVM-15.0.7 (ORCJIT, znver2)
        - Threads: 48 default, 0 interactive, 24 GC (on 2 virtual cores)
        - Environment:
        -   LD_LIBRARY_PATH = /usr/local/nvidia/lib:/usr/local/nvidia/lib64
        -   JULIA_DEPOT_PATH = /root/.cache/julia-buildkite-plugin/depots/01872db4-8c79-43af-ab7d-12abac4f24f6
        -   JULIA_PROJECT = /var/lib/buildkite-agent/builds/gpuci-4/julialang/lux-dot-jl/docs/Project.toml
        -   JULIA_AMDGPU_LOGGING_ENABLED = true
        -   JULIA_DEBUG = Literate
        -   JULIA_CPU_THREADS = 2
        -   JULIA_NUM_THREADS = 48
        -   JULIA_LOAD_PATH = @:@v#.#:@stdlib
        -   JULIA_CUDA_HARD_MEMORY_LIMIT = 25%
        - 
        - ````
        - 
        - ---
        - 
        - *This page was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*
        - 
